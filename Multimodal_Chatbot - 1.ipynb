{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6019eb38",
   "metadata": {},
   "source": [
    "\n",
    "# Retail Product Discovery & Recommendation – Multimodal Chatbot\n",
    "\n",
    "This single notebook implements:\n",
    "- Text-based recommendation (Groq LLM)\n",
    "- Image-based similarity search (CLIP + FAISS)\n",
    "- Voice-based search (Whisper STT)\n",
    "- Interactive UI using Jupyter widgets\n",
    "- Evaluation plots and tables\n",
    "\n",
    "This notebook is designed for **academic submission and viva demonstration**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c654d",
   "metadata": {},
   "source": [
    "## 1. Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run once if required\n",
    "# !pip install ipywidgets torch torchvision torchaudio\n",
    "# !pip install faiss-cpu pandas matplotlib scikit-learn\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "# !pip install openai-whisper soundfile pydub groq python-dotenv\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Image, clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20793bb",
   "metadata": {},
   "source": [
    "## 2. Environment & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9cef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "def extract_intent_entities(text):\n",
    "    text = text.lower()\n",
    "    if \"jacket\" in text:\n",
    "        return \"search\", {\"category\": \"jacket\"}\n",
    "    return \"search\", {}\n",
    "\n",
    "class GroqClient:\n",
    "    def __init__(self):\n",
    "        self.client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "\n",
    "    def chat(self, prompt):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a retail recommendation assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "llm = GroqClient()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37a4789",
   "metadata": {},
   "source": [
    "## 3. Image Embeddings (CLIP) + FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "import faiss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def encode_image(path):\n",
    "    image = preprocess(PILImage.open(path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        return clip_model.encode_image(image).cpu().numpy()\n",
    "\n",
    "class AmazonFashionIndexer:\n",
    "    def __init__(self, dataset_dir):\n",
    "        self.image_dir = os.path.join(dataset_dir, \"images\")\n",
    "        with open(os.path.join(dataset_dir, \"metadata.json\"), \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            self.metadata = json.load(f)\n",
    "        self.embeddings = []\n",
    "        self.products = []\n",
    "\n",
    "    def build(self):\n",
    "        for item in self.metadata:\n",
    "            img_path = os.path.join(self.image_dir, item[\"image\"])\n",
    "            if os.path.exists(img_path):\n",
    "                self.embeddings.append(encode_image(img_path)[0])\n",
    "                self.products.append(item)\n",
    "        self.index = faiss.IndexFlatL2(len(self.embeddings[0]))\n",
    "        self.index.add(np.array(self.embeddings))\n",
    "\n",
    "    def search(self, query, k=5):\n",
    "        distances, indices = self.index.search(query, k)\n",
    "        results = []\n",
    "        seen = set()\n",
    "        for d, i in zip(distances[0], indices[0]):\n",
    "            pid = self.products[i][\"product_id\"]\n",
    "            if pid not in seen:\n",
    "                results.append({\n",
    "                    \"product\": self.products[i],\n",
    "                    \"confidence\": round(100 * (1 / (1 + d)), 2)\n",
    "                })\n",
    "                seen.add(pid)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30afa19f",
   "metadata": {},
   "source": [
    "## 4. Whisper Speech-to-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ccb861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import whisper\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "\n",
    "def transcribe_audio(path):\n",
    "    return whisper_model.transcribe(path)[\"text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0761fc9",
   "metadata": {},
   "source": [
    "## 5. Interactive Widgets (Text, Image, Voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937cda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TEXT SEARCH\n",
    "text_query = widgets.Text(description=\"Query:\", layout=widgets.Layout(width=\"70%\"))\n",
    "text_button = widgets.Button(description=\"Search\", button_style=\"primary\")\n",
    "text_out = widgets.Output()\n",
    "\n",
    "def text_search(b):\n",
    "    with text_out:\n",
    "        clear_output()\n",
    "        intent, entities = extract_intent_entities(text_query.value)\n",
    "        print(llm.chat(f\"User intent: {intent}, entities: {entities}\"))\n",
    "\n",
    "text_button.on_click(text_search)\n",
    "display(widgets.VBox([text_query, text_button, text_out]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d07424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IMAGE SEARCH\n",
    "image_upload = widgets.FileUpload(accept=\".jpg,.png\", multiple=False)\n",
    "img_out = widgets.Output()\n",
    "\n",
    "def image_search(change):\n",
    "    with img_out:\n",
    "        clear_output()\n",
    "        up = list(image_upload.value.values())[0]\n",
    "        with open(\"query.jpg\", \"wb\") as f:\n",
    "            f.write(up[\"content\"])\n",
    "        display(Image(\"query.jpg\", width=200))\n",
    "\n",
    "        indexer = AmazonFashionIndexer(\"data/amazon_fashion\")\n",
    "        indexer.build()\n",
    "        results = indexer.search(encode_image(\"query.jpg\"))\n",
    "\n",
    "        for r in results:\n",
    "            p = r[\"product\"]\n",
    "            display(Image(f\"data/amazon_fashion/images/{p['image']}\", width=180))\n",
    "            print(p[\"category\"], \"| €\", p[\"price\"], \"|\", r[\"confidence\"], \"%\")\n",
    "\n",
    "image_upload.observe(image_search, names=\"value\")\n",
    "display(widgets.VBox([image_upload, img_out]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VOICE SEARCH\n",
    "audio_upload = widgets.FileUpload(accept=\".wav,.mp3\", multiple=False)\n",
    "voice_out = widgets.Output()\n",
    "\n",
    "def voice_search(change):\n",
    "    with voice_out:\n",
    "        clear_output()\n",
    "        up = list(audio_upload.value.values())[0]\n",
    "        with open(\"voice.wav\", \"wb\") as f:\n",
    "            f.write(up[\"content\"])\n",
    "        text = transcribe_audio(\"voice.wav\")\n",
    "        print(\"Recognized:\", text)\n",
    "        print(llm.chat(text))\n",
    "\n",
    "audio_upload.observe(voice_search, names=\"value\")\n",
    "display(widgets.VBox([audio_upload, voice_out]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd4bb8c",
   "metadata": {},
   "source": [
    "## 6. Evaluation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_json(\"data/amazon_fashion/metadata.json\")\n",
    "\n",
    "df[\"price\"] = df[\"price\"].astype(float)\n",
    "\n",
    "plt.figure()\n",
    "df[\"category\"].value_counts().plot(kind=\"bar\", title=\"Category Distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "df[\"gender\"].value_counts().plot(kind=\"bar\", title=\"Gender Distribution\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(df[\"price\"], bins=5)\n",
    "plt.title(\"Price Distribution\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}